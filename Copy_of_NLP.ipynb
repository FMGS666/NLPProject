{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPffg6oWjSDd",
        "outputId": "5046c727-4732-418c-ddc5-90e059c22f02"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\lollo\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\lollo\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\lollo\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers==4.22.2 in c:\\users\\lollo\\anaconda3\\envs\\nlp\\lib\\site-packages (4.22.2)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\lollo\\anaconda3\\envs\\nlp\\lib\\site-packages (from transformers==4.22.2) (23.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in c:\\users\\lollo\\anaconda3\\envs\\nlp\\lib\\site-packages (from transformers==4.22.2) (0.12.1)\n",
            "Requirement already satisfied: filelock in c:\\users\\lollo\\anaconda3\\envs\\nlp\\lib\\site-packages (from transformers==4.22.2) (3.11.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\lollo\\anaconda3\\envs\\nlp\\lib\\site-packages (from transformers==4.22.2) (2023.3.23)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\lollo\\anaconda3\\envs\\nlp\\lib\\site-packages (from transformers==4.22.2) (1.23.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\lollo\\anaconda3\\envs\\nlp\\lib\\site-packages (from transformers==4.22.2) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.9.0 in c:\\users\\lollo\\anaconda3\\envs\\nlp\\lib\\site-packages (from transformers==4.22.2) (0.13.4)\n",
            "Requirement already satisfied: requests in c:\\users\\lollo\\anaconda3\\envs\\nlp\\lib\\site-packages (from transformers==4.22.2) (2.28.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\users\\lollo\\anaconda3\\envs\\nlp\\lib\\site-packages (from transformers==4.22.2) (4.65.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\lollo\\anaconda3\\envs\\nlp\\lib\\site-packages (from huggingface-hub<1.0,>=0.9.0->transformers==4.22.2) (4.5.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\lollo\\anaconda3\\envs\\nlp\\lib\\site-packages (from tqdm>=4.27->transformers==4.22.2) (0.4.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lollo\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests->transformers==4.22.2) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lollo\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests->transformers==4.22.2) (3.1.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\lollo\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests->transformers==4.22.2) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lollo\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests->transformers==4.22.2) (3.4)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "!pip install transformers==4.22.2\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "IyJcS4PCSdLF"
      },
      "outputs": [],
      "source": [
        "path = r'.\\data\\traindata.csv'\n",
        "pathdev = r'.\\data/devdata.csv'\n",
        "df = pd.read_csv(path,sep='\\t', header=None)\n",
        "df_dev = pd.read_csv(pathdev,sep='\\t', header=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Aes0xZR8X8cr"
      },
      "outputs": [],
      "source": [
        "df.columns = ['polarity', 'aspect_category', 'target_term', 'character_offsets', 'sentence']\n",
        "df_dev.columns = ['polarity', 'aspect_category', 'target_term', 'character_offsets', 'sentence']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "1FR15AaxW4Ty",
        "outputId": "b23973c0-bc76-4537-8ab7-8d80d32604bb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>polarity</th>\n",
              "      <th>aspect_category</th>\n",
              "      <th>target_term</th>\n",
              "      <th>character_offsets</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>positive</td>\n",
              "      <td>AMBIENCE#GENERAL</td>\n",
              "      <td>seating</td>\n",
              "      <td>18:25</td>\n",
              "      <td>short and sweet – seating is great:it's romant...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>positive</td>\n",
              "      <td>AMBIENCE#GENERAL</td>\n",
              "      <td>trattoria</td>\n",
              "      <td>25:34</td>\n",
              "      <td>This quaint and romantic trattoria is at the t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>positive</td>\n",
              "      <td>FOOD#QUALITY</td>\n",
              "      <td>food</td>\n",
              "      <td>98:102</td>\n",
              "      <td>The have over 100 different beers to offer thi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>negative</td>\n",
              "      <td>SERVICE#GENERAL</td>\n",
              "      <td>STAFF</td>\n",
              "      <td>5:10</td>\n",
              "      <td>THIS STAFF SHOULD BE FIRED.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>positive</td>\n",
              "      <td>FOOD#STYLE_OPTIONS</td>\n",
              "      <td>menu</td>\n",
              "      <td>4:8</td>\n",
              "      <td>The menu looked great, and the waiter was very...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   polarity     aspect_category target_term character_offsets  \\\n",
              "0  positive    AMBIENCE#GENERAL     seating             18:25   \n",
              "1  positive    AMBIENCE#GENERAL   trattoria             25:34   \n",
              "2  positive        FOOD#QUALITY        food            98:102   \n",
              "3  negative     SERVICE#GENERAL       STAFF              5:10   \n",
              "4  positive  FOOD#STYLE_OPTIONS        menu               4:8   \n",
              "\n",
              "                                            sentence  \n",
              "0  short and sweet – seating is great:it's romant...  \n",
              "1  This quaint and romantic trattoria is at the t...  \n",
              "2  The have over 100 different beers to offer thi...  \n",
              "3                        THIS STAFF SHOULD BE FIRED.  \n",
              "4  The menu looked great, and the waiter was very...  "
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8H8BaRGaiP5",
        "outputId": "b6a52461-d01f-4039-c55c-a0a00e14c9f2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "positive    1055\n",
              "negative     390\n",
              "neutral       58\n",
              "Name: polarity, dtype: int64"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.polarity.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErAdFbSFBObU",
        "outputId": "5a2608fc-93da-4bb0-be43-d5c77a782d41"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "440"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.sentence.duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "bRjrLmoAcPIA"
      },
      "outputs": [],
      "source": [
        "#label encoding the polarity\n",
        "lb = preprocessing.LabelEncoder()\n",
        "df['polarity']=lb.fit_transform(df['polarity'])\n",
        "df_dev['polarity']=lb.fit_transform(df_dev['polarity'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "R4htwqDpv92p"
      },
      "outputs": [],
      "source": [
        "def add_sep(text,offset):\n",
        "    sep1 = ' <target_bos> '\n",
        "    sep2 = ' <target_eos> '\n",
        "    split = offset.split(':')\n",
        "    sentence_left = text[:int(split[0])]\n",
        "    target = text[int(split[0]):int(split[1])]\n",
        "    sentence_right = text[int(split[1]):]\n",
        "    concat = sentence_left+sep1+target+sep2+sentence_right\n",
        "    return concat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "KBoQyaoDy29V"
      },
      "outputs": [],
      "source": [
        "def aspect_categories(aspect):\n",
        "    \n",
        "  if aspect == 'AMBIENCE#GENERAL':\n",
        "    return \"What do you think of the <target_bos> ambience <target_eos> ?\"\n",
        "\n",
        "  elif aspect == 'FOOD#QUALITY':\n",
        "    return \"What do you think of the <target_bos> food quality <target_eos> ?\"\n",
        "\n",
        "  elif aspect == 'SERVICE#GENERAL':\n",
        "    return \"What do you think of the <target_bos> service <target_eos> ?\"\n",
        "\n",
        "  elif aspect == 'FOOD#STYLE_OPTIONS':\n",
        "    return \"What do you think of the <target_bos> food choices <target_eos> ?\"\n",
        "\n",
        "  elif aspect == 'DRINKS#QUALITY':\n",
        "    return \"What do you think of the <target_bos> drinks quality <target_eos> ?\"\n",
        "\n",
        "  elif aspect == 'RESTAURANT#MISCELLANEOUS' or aspect == 'RESTAURANT#GENERAL':\n",
        "    return \"What do you think of the <target_bos> restaurant <target_eos> ?\"\n",
        "\n",
        "  elif aspect == 'LOCATION#GENERAL':\n",
        "    return 'What do you think of the <target_bos> location <target_eos> ?'\n",
        "\n",
        "  elif aspect == 'DRINKS#STYLE_OPTIONS':\n",
        "    return \"What do you think of the <target_bos> drink choices <target_eos> ?\"\n",
        "  \n",
        "  else:\n",
        "    return 'What do you think of the <target_bos> price <target_eos> ?'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        },
        "id": "wSk44gEB1mca",
        "outputId": "a10bbf95-1650-4ea4-9380-ff9726920dd8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>polarity</th>\n",
              "      <th>aspect_category</th>\n",
              "      <th>target_term</th>\n",
              "      <th>character_offsets</th>\n",
              "      <th>sentence</th>\n",
              "      <th>sentence_sep</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>AMBIENCE#GENERAL</td>\n",
              "      <td>seating</td>\n",
              "      <td>18:25</td>\n",
              "      <td>short and sweet – seating is great:it's romant...</td>\n",
              "      <td>short and sweet –  &lt;target_bos&gt; seating &lt;targe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>AMBIENCE#GENERAL</td>\n",
              "      <td>trattoria</td>\n",
              "      <td>25:34</td>\n",
              "      <td>This quaint and romantic trattoria is at the t...</td>\n",
              "      <td>This quaint and romantic  &lt;target_bos&gt; trattor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>FOOD#QUALITY</td>\n",
              "      <td>food</td>\n",
              "      <td>98:102</td>\n",
              "      <td>The have over 100 different beers to offer thi...</td>\n",
              "      <td>The have over 100 different beers to offer thi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>SERVICE#GENERAL</td>\n",
              "      <td>STAFF</td>\n",
              "      <td>5:10</td>\n",
              "      <td>THIS STAFF SHOULD BE FIRED.</td>\n",
              "      <td>THIS  &lt;target_bos&gt; STAFF &lt;target_eos&gt;  SHOULD ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>FOOD#STYLE_OPTIONS</td>\n",
              "      <td>menu</td>\n",
              "      <td>4:8</td>\n",
              "      <td>The menu looked great, and the waiter was very...</td>\n",
              "      <td>The  &lt;target_bos&gt; menu &lt;target_eos&gt;  looked gr...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   polarity     aspect_category target_term character_offsets  \\\n",
              "0         2    AMBIENCE#GENERAL     seating             18:25   \n",
              "1         2    AMBIENCE#GENERAL   trattoria             25:34   \n",
              "2         2        FOOD#QUALITY        food            98:102   \n",
              "3         0     SERVICE#GENERAL       STAFF              5:10   \n",
              "4         2  FOOD#STYLE_OPTIONS        menu               4:8   \n",
              "\n",
              "                                            sentence  \\\n",
              "0  short and sweet – seating is great:it's romant...   \n",
              "1  This quaint and romantic trattoria is at the t...   \n",
              "2  The have over 100 different beers to offer thi...   \n",
              "3                        THIS STAFF SHOULD BE FIRED.   \n",
              "4  The menu looked great, and the waiter was very...   \n",
              "\n",
              "                                        sentence_sep  \n",
              "0  short and sweet –  <target_bos> seating <targe...  \n",
              "1  This quaint and romantic  <target_bos> trattor...  \n",
              "2  The have over 100 different beers to offer thi...  \n",
              "3  THIS  <target_bos> STAFF <target_eos>  SHOULD ...  \n",
              "4  The  <target_bos> menu <target_eos>  looked gr...  "
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['sentence_sep'] = df.apply(lambda x: add_sep(x['sentence'],x['character_offsets']),axis = 1)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "voA3smPU4ghQ"
      },
      "outputs": [],
      "source": [
        "df_dev['sentence_sep'] = df_dev.apply(lambda x: add_sep(x['sentence'],x['character_offsets']),axis = 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "TmLt7aBDzCmd"
      },
      "outputs": [],
      "source": [
        "df['aspect_category'] = df.apply(lambda x: aspect_categories(x['aspect_category']),axis=1)\n",
        "df_dev['aspect_category'] = df_dev.apply(lambda x: aspect_categories(x['aspect_category']),axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "iS-RAF5W05H-"
      },
      "outputs": [],
      "source": [
        "df['input'] = df.apply(lambda x: f\"{x['aspect_category']} {x['sentence_sep']}\", axis=1)\n",
        "df_dev['input'] = df_dev.apply(lambda x: f\"{x['aspect_category']} {x['sentence_sep']}\", axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        },
        "id": "CBWBCUvZydu4",
        "outputId": "3d39f0ac-6f41-4051-ad5a-346fdc0d2cbe"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>polarity</th>\n",
              "      <th>aspect_category</th>\n",
              "      <th>target_term</th>\n",
              "      <th>character_offsets</th>\n",
              "      <th>sentence</th>\n",
              "      <th>sentence_sep</th>\n",
              "      <th>input</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>What do you think of the &lt;target_bos&gt; ambience...</td>\n",
              "      <td>seating</td>\n",
              "      <td>18:25</td>\n",
              "      <td>short and sweet – seating is great:it's romant...</td>\n",
              "      <td>short and sweet –  &lt;target_bos&gt; seating &lt;targe...</td>\n",
              "      <td>What do you think of the &lt;target_bos&gt; ambience...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>What do you think of the &lt;target_bos&gt; ambience...</td>\n",
              "      <td>trattoria</td>\n",
              "      <td>25:34</td>\n",
              "      <td>This quaint and romantic trattoria is at the t...</td>\n",
              "      <td>This quaint and romantic  &lt;target_bos&gt; trattor...</td>\n",
              "      <td>What do you think of the &lt;target_bos&gt; ambience...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>What do you think of the &lt;target_bos&gt; food qua...</td>\n",
              "      <td>food</td>\n",
              "      <td>98:102</td>\n",
              "      <td>The have over 100 different beers to offer thi...</td>\n",
              "      <td>The have over 100 different beers to offer thi...</td>\n",
              "      <td>What do you think of the &lt;target_bos&gt; food qua...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>What do you think of the &lt;target_bos&gt; service ...</td>\n",
              "      <td>STAFF</td>\n",
              "      <td>5:10</td>\n",
              "      <td>THIS STAFF SHOULD BE FIRED.</td>\n",
              "      <td>THIS  &lt;target_bos&gt; STAFF &lt;target_eos&gt;  SHOULD ...</td>\n",
              "      <td>What do you think of the &lt;target_bos&gt; service ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>What do you think of the &lt;target_bos&gt; food cho...</td>\n",
              "      <td>menu</td>\n",
              "      <td>4:8</td>\n",
              "      <td>The menu looked great, and the waiter was very...</td>\n",
              "      <td>The  &lt;target_bos&gt; menu &lt;target_eos&gt;  looked gr...</td>\n",
              "      <td>What do you think of the &lt;target_bos&gt; food cho...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   polarity                                    aspect_category target_term  \\\n",
              "0         2  What do you think of the <target_bos> ambience...     seating   \n",
              "1         2  What do you think of the <target_bos> ambience...   trattoria   \n",
              "2         2  What do you think of the <target_bos> food qua...        food   \n",
              "3         0  What do you think of the <target_bos> service ...       STAFF   \n",
              "4         2  What do you think of the <target_bos> food cho...        menu   \n",
              "\n",
              "  character_offsets                                           sentence  \\\n",
              "0             18:25  short and sweet – seating is great:it's romant...   \n",
              "1             25:34  This quaint and romantic trattoria is at the t...   \n",
              "2            98:102  The have over 100 different beers to offer thi...   \n",
              "3              5:10                        THIS STAFF SHOULD BE FIRED.   \n",
              "4               4:8  The menu looked great, and the waiter was very...   \n",
              "\n",
              "                                        sentence_sep  \\\n",
              "0  short and sweet –  <target_bos> seating <targe...   \n",
              "1  This quaint and romantic  <target_bos> trattor...   \n",
              "2  The have over 100 different beers to offer thi...   \n",
              "3  THIS  <target_bos> STAFF <target_eos>  SHOULD ...   \n",
              "4  The  <target_bos> menu <target_eos>  looked gr...   \n",
              "\n",
              "                                               input  \n",
              "0  What do you think of the <target_bos> ambience...  \n",
              "1  What do you think of the <target_bos> ambience...  \n",
              "2  What do you think of the <target_bos> food qua...  \n",
              "3  What do you think of the <target_bos> service ...  \n",
              "4  What do you think of the <target_bos> food cho...  "
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "R1-kXH2Mzddb"
      },
      "outputs": [],
      "source": [
        "# set the device to run the model on (GPU or CPU)\n",
        "import torch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C25hIoHthSWF"
      },
      "source": [
        "## Roberta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0c1McxmmBw-",
        "outputId": "7e301b86-feae-499e-b6c4-a1594297dc39"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "from transformers import BertTokenizer, RobertaTokenizer, RobertaForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from transformers import BertModel, RobertaModel, RobertaConfig\n",
        "\n",
        "\n",
        "\n",
        "# initialize the RoBERTa tokenizer and model\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "\n",
        "\n",
        "roberta = RobertaModel.from_pretrained('roberta-base').to(device)\n",
        "\n",
        "class CustomRobertaModel(torch.nn.Module):\n",
        "    def __init__(self, roberta):\n",
        "        super(CustomRobertaModel, self).__init__()\n",
        "        self.roberta = roberta\n",
        "        self.linear1 = torch.nn.Linear(768, 768)\n",
        "        #self.relu = torch.nn.ReLU()\n",
        "        self.dropout = torch.nn.Dropout(0.1)\n",
        "        self.linear2 = torch.nn.Linear(768, 3)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        output = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = output.pooler_output\n",
        "        pooled_output = self.linear1(pooled_output)\n",
        "        #pooled_output = self.relu(pooled_output)\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.linear2(pooled_output)\n",
        "        return logits\n",
        "\n",
        "model = CustomRobertaModel(roberta).to(device)\n",
        "# combine the RoBERTa model and the classifier head into a single model\n",
        "\n",
        "# tokenize the sentences in the training and validation sets\n",
        "train_tokens = tokenizer.batch_encode_plus(df['input'].tolist(),\n",
        "                                           max_length =128,\n",
        "                                           padding='max_length',\n",
        "                                           add_special_tokens = True,\n",
        "                                           truncation=True, \n",
        "                                           return_tensors='pt')\n",
        "val_tokens = tokenizer.batch_encode_plus(df_dev['input'].tolist(),\n",
        "                                         max_length =128,\n",
        "                                         padding='max_length',\n",
        "                                         add_special_tokens = True,\n",
        "                                         truncation=True, \n",
        "                                         return_tensors='pt')\n",
        "\n",
        "# create PyTorch DataLoader objects for the training and validation sets\n",
        "train_dataset = TensorDataset(train_tokens['input_ids'], train_tokens['attention_mask'],torch.tensor(df['polarity'].tolist()))\n",
        "val_dataset = TensorDataset(val_tokens['input_ids'], val_tokens['attention_mask'],torch.tensor(df_dev['polarity'].tolist()))\n",
        "train_loader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=64)\n",
        "val_loader = DataLoader(val_dataset, sampler=SequentialSampler(val_dataset), batch_size=32)\n",
        "\n",
        "# set the optimizer and learning rate scheduler\n",
        "epochs = 20\n",
        "# define the class weights as a tensor\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(df['polarity']), y=df['polarity'])\n",
        "class_weights = torch.FloatTensor(class_weights).to(device)\n",
        "\n",
        "# define the loss function with the weighted cross entropy loss\n",
        "loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_loader)*epochs)\n",
        "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-M0Av7AvmMgo",
        "outputId": "1dd9cacc-fdda-4ad5-9858-4b51bf8c6c89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "logits torch.Size([64, 3]) labels torch.Size([64])\n",
            "logits torch.Size([64, 3]) labels torch.Size([64])\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[17], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(logits, labels)\n\u001b[0;32m     14\u001b[0m train_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[1;32m---> 15\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     16\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     17\u001b[0m scheduler\u001b[39m.\u001b[39mstep()\n",
            "File \u001b[1;32mc:\\Users\\lollo\\anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    487\u001b[0m     )\n\u001b[1;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    490\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\lollo\\anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# define the training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for batch in train_loader:\n",
        "        input_ids, attention_mask,  labels = batch\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "        labels = labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        print(f\"logits {logits.size()} labels {labels.size()}\")\n",
        "        loss = loss_fn(logits, labels)\n",
        "        train_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "    train_loss /= len(train_loader)\n",
        "    \n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    val_preds = []\n",
        "    val_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids, attention_mask, labels = batch\n",
        "            input_ids = input_ids.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "            labels = labels.to(device)\n",
        "            logits = model(input_ids, attention_mask=attention_mask)\n",
        "            loss = loss_fn(logits, labels)\n",
        "            val_loss += loss.item() * input_ids.size(0)\n",
        "            val_preds += torch.argmax(logits, axis=1).tolist()\n",
        "            val_labels += labels.tolist()\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "    val_accuracy = accuracy_score(val_labels, val_preds)\n",
        "    val_precision = precision_score(val_labels, val_preds, average='weighted', zero_division=1)\n",
        "    val_recall = recall_score(val_labels, val_preds, average='weighted')\n",
        "    val_f1 = f1_score(val_labels, val_preds, average='weighted')\n",
        "    report = classification_report(val_labels, val_preds, labels=[0, 1, 2], target_names=['Negative', 'Neutral', 'Positive'])\n",
        "    print(f'Epoch {epoch+1} - Train Loss: {train_loss:.4f} - Val Loss: {val_loss:.4f} - Val Accuracy: {val_accuracy:.4f} - Val Precision: {val_precision:.4f} - Val Recall: {val_recall:.4f} - Val F1: {val_f1:4f}')\n",
        "    print(report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxkDi_pdB_Jk",
        "outputId": "32e4cd20-881d-445e-9bc2-1f69e81ba68d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from transformers import RobertaModel, RobertaConfig\n",
        "\n",
        "\n",
        "\n",
        "# initialize the RoBERTa tokenizer and model\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-large')\n",
        "\n",
        "\n",
        "roberta = RobertaModel.from_pretrained('roberta-large').to(device)\n",
        "\n",
        "class CustomRobertaModel(torch.nn.Module):\n",
        "    def __init__(self, roberta):\n",
        "        super(CustomRobertaModel, self).__init__()\n",
        "        self.roberta = roberta\n",
        "        self.linear1 = torch.nn.Linear(1024, 1024)\n",
        "        self.dropout = torch.nn.Dropout(0.1)\n",
        "        self.linear2 = torch.nn.Linear(1024, 3)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        output = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = output.pooler_output\n",
        "        pooled_output = self.linear1(pooled_output)\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.linear2(pooled_output)\n",
        "        return logits\n",
        "\n",
        "model = CustomRobertaModel(roberta).to(device)\n",
        "# combine the RoBERTa model and the classifier head into a single model\n",
        "\n",
        "# tokenize the sentences in the training and validation sets\n",
        "train_tokens = tokenizer.batch_encode_plus(df['input'].tolist(),\n",
        "                                           max_length =128,\n",
        "                                           padding='max_length',\n",
        "                                           add_special_tokens = True,\n",
        "                                           truncation=True, \n",
        "                                           return_tensors='pt')\n",
        "val_tokens = tokenizer.batch_encode_plus(df_dev['input'].tolist(),\n",
        "                                         max_length =128,\n",
        "                                         padding='max_length',\n",
        "                                         add_special_tokens = True,\n",
        "                                         truncation=True, \n",
        "                                         return_tensors='pt')\n",
        "\n",
        "# create PyTorch DataLoader objects for the training and validation sets\n",
        "train_dataset = TensorDataset(train_tokens['input_ids'], train_tokens['attention_mask'], torch.tensor(df['polarity'].tolist()))\n",
        "val_dataset = TensorDataset(val_tokens['input_ids'], val_tokens['attention_mask'], torch.tensor(df_dev['polarity'].tolist()))\n",
        "train_loader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=16)\n",
        "val_loader = DataLoader(val_dataset, sampler=SequentialSampler(val_dataset), batch_size=8)\n",
        "\n",
        "# set the optimizer and learning rate scheduler\n",
        "epochs = 15\n",
        "# define the class weights as a tensor\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(df['polarity']), y=df['polarity'])\n",
        "class_weights = torch.FloatTensor(class_weights).to(device)\n",
        "#class_weights = torch.tensor([3., 20., 1.])\n",
        "# define the loss function with the weighted cross entropy loss\n",
        "loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_loader)*epochs)\n",
        "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x__1jby7p66Z",
        "outputId": "5134474b-a079-4ddc-8359-54377baed431"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 - Val Loss: 1.0272 - Val Accuracy: 0.7261 - Val Precision: 0.8443 - Val Recall: 0.7261 - Val F1: 0.730031\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.49      0.95      0.65        98\n",
            "     Neutral       0.00      0.00      0.00        14\n",
            "    Positive       0.97      0.68      0.80       264\n",
            "\n",
            "    accuracy                           0.73       376\n",
            "   macro avg       0.49      0.54      0.48       376\n",
            "weighted avg       0.81      0.73      0.73       376\n",
            "\n",
            "Epoch 2 - Val Loss: 0.8083 - Val Accuracy: 0.8617 - Val Precision: 0.8538 - Val Recall: 0.8617 - Val F1: 0.856194\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.73      0.86      0.79        98\n",
            "     Neutral       0.00      0.00      0.00        14\n",
            "    Positive       0.94      0.91      0.93       264\n",
            "\n",
            "    accuracy                           0.86       376\n",
            "   macro avg       0.56      0.59      0.57       376\n",
            "weighted avg       0.85      0.86      0.86       376\n",
            "\n",
            "Epoch 3 - Val Loss: 0.5043 - Val Accuracy: 0.9069 - Val Precision: 0.9130 - Val Recall: 0.9069 - Val F1: 0.909400\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.88      0.84      0.86        98\n",
            "     Neutral       0.42      0.57      0.48        14\n",
            "    Positive       0.95      0.95      0.95       264\n",
            "\n",
            "    accuracy                           0.91       376\n",
            "   macro avg       0.75      0.79      0.76       376\n",
            "weighted avg       0.91      0.91      0.91       376\n",
            "\n",
            "Epoch 4 - Val Loss: 0.6245 - Val Accuracy: 0.7553 - Val Precision: 0.9245 - Val Recall: 0.7553 - Val F1: 0.815359\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.92      0.58      0.71        98\n",
            "     Neutral       0.13      0.86      0.23        14\n",
            "    Positive       0.97      0.81      0.88       264\n",
            "\n",
            "    accuracy                           0.76       376\n",
            "   macro avg       0.67      0.75      0.61       376\n",
            "weighted avg       0.92      0.76      0.82       376\n",
            "\n",
            "Epoch 5 - Val Loss: 0.5518 - Val Accuracy: 0.8723 - Val Precision: 0.9000 - Val Recall: 0.8723 - Val F1: 0.882872\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.83      0.81      0.82        98\n",
            "     Neutral       0.32      0.71      0.44        14\n",
            "    Positive       0.96      0.91      0.93       264\n",
            "\n",
            "    accuracy                           0.87       376\n",
            "   macro avg       0.70      0.81      0.73       376\n",
            "weighted avg       0.90      0.87      0.88       376\n",
            "\n",
            "Epoch 6 - Val Loss: 0.8632 - Val Accuracy: 0.6888 - Val Precision: 0.8265 - Val Recall: 0.6888 - Val F1: 0.743202\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.75      0.53      0.62        98\n",
            "     Neutral       0.08      0.50      0.14        14\n",
            "    Positive       0.89      0.76      0.82       264\n",
            "\n",
            "    accuracy                           0.69       376\n",
            "   macro avg       0.58      0.60      0.53       376\n",
            "weighted avg       0.83      0.69      0.74       376\n",
            "\n",
            "Epoch 7 - Val Loss: 0.6397 - Val Accuracy: 0.8777 - Val Precision: 0.8961 - Val Recall: 0.8777 - Val F1: 0.884965\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.86      0.79      0.82        98\n",
            "     Neutral       0.31      0.57      0.40        14\n",
            "    Positive       0.94      0.93      0.94       264\n",
            "\n",
            "    accuracy                           0.88       376\n",
            "   macro avg       0.70      0.76      0.72       376\n",
            "weighted avg       0.90      0.88      0.88       376\n",
            "\n",
            "Epoch 8 - Val Loss: 0.7092 - Val Accuracy: 0.9043 - Val Precision: 0.9058 - Val Recall: 0.9043 - Val F1: 0.904821\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.83      0.88      0.86        98\n",
            "     Neutral       0.57      0.57      0.57        14\n",
            "    Positive       0.95      0.93      0.94       264\n",
            "\n",
            "    accuracy                           0.90       376\n",
            "   macro avg       0.79      0.79      0.79       376\n",
            "weighted avg       0.91      0.90      0.90       376\n",
            "\n",
            "Epoch 9 - Val Loss: 0.6684 - Val Accuracy: 0.8697 - Val Precision: 0.8968 - Val Recall: 0.8697 - Val F1: 0.879927\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.86      0.77      0.81        98\n",
            "     Neutral       0.29      0.64      0.40        14\n",
            "    Positive       0.94      0.92      0.93       264\n",
            "\n",
            "    accuracy                           0.87       376\n",
            "   macro avg       0.70      0.78      0.71       376\n",
            "weighted avg       0.90      0.87      0.88       376\n",
            "\n",
            "Epoch 10 - Val Loss: 0.6982 - Val Accuracy: 0.9069 - Val Precision: 0.9054 - Val Recall: 0.9069 - Val F1: 0.906016\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.87      0.85      0.86        98\n",
            "     Neutral       0.54      0.50      0.52        14\n",
            "    Positive       0.94      0.95      0.94       264\n",
            "\n",
            "    accuracy                           0.91       376\n",
            "   macro avg       0.78      0.77      0.77       376\n",
            "weighted avg       0.91      0.91      0.91       376\n",
            "\n",
            "Epoch 11 - Val Loss: 0.7256 - Val Accuracy: 0.9069 - Val Precision: 0.9055 - Val Recall: 0.9069 - Val F1: 0.905496\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.83      0.87      0.85        98\n",
            "     Neutral       0.70      0.50      0.58        14\n",
            "    Positive       0.94      0.94      0.94       264\n",
            "\n",
            "    accuracy                           0.91       376\n",
            "   macro avg       0.83      0.77      0.79       376\n",
            "weighted avg       0.91      0.91      0.91       376\n",
            "\n",
            "Epoch 12 - Val Loss: 0.6756 - Val Accuracy: 0.9096 - Val Precision: 0.9111 - Val Recall: 0.9096 - Val F1: 0.910289\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.87      0.88      0.87        98\n",
            "     Neutral       0.47      0.50      0.48        14\n",
            "    Positive       0.95      0.94      0.95       264\n",
            "\n",
            "    accuracy                           0.91       376\n",
            "   macro avg       0.76      0.77      0.77       376\n",
            "weighted avg       0.91      0.91      0.91       376\n",
            "\n",
            "Epoch 13 - Val Loss: 0.7168 - Val Accuracy: 0.9176 - Val Precision: 0.9174 - Val Recall: 0.9176 - Val F1: 0.917361\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.86      0.90      0.88        98\n",
            "     Neutral       0.54      0.50      0.52        14\n",
            "    Positive       0.96      0.95      0.95       264\n",
            "\n",
            "    accuracy                           0.92       376\n",
            "   macro avg       0.79      0.78      0.78       376\n",
            "weighted avg       0.92      0.92      0.92       376\n",
            "\n",
            "Epoch 14 - Val Loss: 0.7240 - Val Accuracy: 0.9149 - Val Precision: 0.9134 - Val Recall: 0.9149 - Val F1: 0.914015\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.86      0.88      0.87        98\n",
            "     Neutral       0.58      0.50      0.54        14\n",
            "    Positive       0.95      0.95      0.95       264\n",
            "\n",
            "    accuracy                           0.91       376\n",
            "   macro avg       0.80      0.78      0.79       376\n",
            "weighted avg       0.91      0.91      0.91       376\n",
            "\n",
            "Epoch 15 - Val Loss: 0.7233 - Val Accuracy: 0.9096 - Val Precision: 0.9087 - Val Recall: 0.9096 - Val F1: 0.909116\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.86      0.87      0.86        98\n",
            "     Neutral       0.54      0.50      0.52        14\n",
            "    Positive       0.95      0.95      0.95       264\n",
            "\n",
            "    accuracy                           0.91       376\n",
            "   macro avg       0.78      0.77      0.78       376\n",
            "weighted avg       0.91      0.91      0.91       376\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# define the training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for batch in train_loader:\n",
        "        input_ids, attention_mask, labels = batch\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "        labels = labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        loss = loss_fn(logits, labels)\n",
        "        train_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "    train_loss /= len(train_loader)\n",
        "    \n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    val_preds = []\n",
        "    val_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids, attention_mask, labels = batch\n",
        "            input_ids = input_ids.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "            labels = labels.to(device)\n",
        "            logits = model(input_ids, attention_mask=attention_mask)\n",
        "            loss = loss_fn(logits, labels)\n",
        "            val_loss += loss.item() * input_ids.size(0)\n",
        "            val_preds += torch.argmax(logits, axis=1).tolist()\n",
        "            val_labels += labels.tolist()\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "    val_accuracy = accuracy_score(val_labels, val_preds)\n",
        "    val_precision = precision_score(val_labels, val_preds, average='weighted', zero_division=1)\n",
        "    val_recall = recall_score(val_labels, val_preds, average='weighted')\n",
        "    val_f1 = f1_score(val_labels, val_preds, average='weighted')\n",
        "    report = classification_report(val_labels, val_preds, labels=[0, 1, 2], target_names=['Negative', 'Neutral', 'Positive'])\n",
        "    print(f'Epoch {epoch+1} - Val Loss: {val_loss:.4f} - Val Accuracy: {val_accuracy:.4f} - Val Precision: {val_precision:.4f} - Val Recall: {val_recall:.4f} - Val F1: {val_f1:4f}')\n",
        "    print(report)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "nlp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10 | packaged by Anaconda, Inc. | (main, Mar 21 2023, 18:39:17) [MSC v.1916 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "c0a9e5bae44e672b476879751bf51c575e04abec7daed337aca60f8ef850c1d8"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
